"""This script contains code that allows a use the raw BERT emeddings in order to
train a classification model based on the same. The code below provides that allows 
the user to train a pairwise sentence classifier without getting into too much detail.

Example
-------
>>> import pandas as pd
>>> from bert_pairwise import encode_sentence_pair, extract_bert_embeddings, train_classifier
>>> RETAIL_DESC = 'ONLINE DESCRIPTION'
>>> LABEL = 'tag'

# Training the classifier
>>> classifier, test_set = train_classifier(train_df, LABEL, train_features)

# Using BERT and trained classifier
>>> vs_data = pd.read_excel("vs_pair_data.xlsx")
>>> vs_input_ids, vs_attn_mask = encode_sentence_pair(vs_data, RETAIL_DESC, 'clean_prod_name', tokenizer, 46)
>>> vs_features = extract_bert_embeddings(vs_input_ids, vs_attn_mask, model, 1000)
>>> vs_data['predictions'] = classifier.predict(vs_features)
>>> vs_data['predict_proba'] = classifier.predict_proba(vs_features)[:,1]
"""
import numpy as np
import pandas as pd
import torch
import tqdm
import transformers
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, train_test_split


def encode_sentence_pair(
    data: pd.DataFrame, sent1: str, sent2: str, tokenizer, padding: int = None
):
    """Takes in a dataframe as input with columns containing
    fields to be used as sentence pairs. Returns input_ids tensor and
    the attention mask. If padding MAX_LEN is the default, however, please
    keep the padding the same while training and testing."""

    assert (sent1 in data) and (sent2 in data), "Fields not present in DF."

    # Tokenization
    tokenized1 = data[sent1].apply(
        (lambda x: tokenizer.encode(x, add_special_tokens=True))
    )
    tokenized2 = data[sent2].apply(
        (lambda x: tokenizer.encode(x, add_special_tokens=True))
    )

    # Bad code, please fix later
    sentences = []
    for i in range(len(data)):
        sentences.append(tokenized1.iloc[i] + tokenized2.iloc[i][1:])

    if not padding:
        max_len = 0
        for i in sentences:
            if len(i) > max_len:
                max_len = len(i)
        print(f"Note: Maximum length {max_len} chosen as padding.")
        padding = max_len

    # HACK: Will clip the sentence to the padding
    # FIXME: Hack is to prevent the code from breaking when len(i) > padding
    sentences = [sentence[:padding] for sentence in sentences]
    padded = np.array([i + [0] * (padding - len(i)) for i in sentences])
    attention_mask = np.where(padded != 0, 1, 0)
    input_ids = torch.tensor(padded)
    attention_mask_ = torch.tensor(attention_mask)
    return input_ids, attention_mask_


def extract_bert_embeddings(input_ids, attention_mask, model, batch_size: int):

    n_dim = 768  # Dimensionality of embeddings returned by BERT
    n_records = len(input_ids)
    n_iters = n_records // batch_size

    print(f"# of iterations: {n_iters}")

    # Initializing result tensor
    features = torch.zeros((0, n_dim))

    with torch.no_grad():
        for idx in tqdm.notebook.tqdm(range(0, n_records, batch_size)):
            last_hidden_states = model(
                input_ids[idx : idx + batch_size],
                attention_mask=attention_mask[idx : idx + batch_size],
            )
            features = torch.cat((features, last_hidden_states[0][:, 0, :]))

    return features


def train_classifier(
    data, label_col, features, clf=LogisticRegression(), random_state: int = 1978
):
    """Function that trains a classifier and returns a trained model object along
    with the predictions on the test_set.

    data: pd.DataFrame
      Data with original records

    label_col:str
      Field that contains the target variable

    features: 2D array-like
        An array containing feature. In this script it is generally the embeddings
        generated by the BERT model.

    clf: sklearn classfier
      Initialized classfier. Logistic Regression by default

    random_state: int
      Random state
    """
    labels = data[label_col]
    train_features, test_features, train_labels, test_labels = train_test_split(
        features, labels, random_state=random_state
    )
    clf.fit(train_features, train_labels)
    print(f"Generating classfication report: ")

    preds = clf.predict(test_features)
    print(classification_report(y_true=test_labels, y_pred=preds))
    a, test_df, b, test_label_check = train_test_split(
        data, labels, random_state=random_state
    )

    # Sanity check
    assert (test_df[label_col] == test_label_check).all()
    assert "prediction" not in test_df
    test_df["prediction"] = preds
    test_df["predict_proba"] = clf.predict_proba(test_features)[:, 1]
    return clf, test_df
